{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e61cd7da",
   "metadata": {},
   "source": [
    "<h2><center> <font color='cyan'> Workflow for UCRA </font> </center></h2>\n",
    "<br>\n",
    "<font color='cyan'> Create a workflow comprising of section for each notebook </font> \n",
    "<h3><center> <font color='cyan'>Table of Contents</font>   </center></h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a75cf9c",
   "metadata": {},
   "source": [
    "[Preface:   Paths and Visual Inspection](#section0)\n",
    "<br>\n",
    "[Section 1: Urban land and built-up area data processing](#section1)\n",
    "<br>\n",
    "[Section 2: Process Sea level rise: Clip and reproject the rasters](#section2) \n",
    "<br>\n",
    "[Section 3: Process CCKP Vars: Exports summary statistics](#section3) \n",
    "<br>\n",
    "[Section 4: Process  historical SPEI data: Exports summary statistics](#section4) \n",
    "<br>\n",
    "[Section 5: Process PM2.5 data : Exports summary statistics](#section5) \n",
    "<br>\n",
    "[Section 6: Process heat increase due to urban land expansion : Exports summary statistics](#section6) \n",
    "<br>\n",
    "[Section 7: Process Landslide Data : Exports summary statistics](#section7) \n",
    "<br>\n",
    "[Section 8: Process drought data : Exports summary statistics](#section8) \n",
    "<br>\n",
    "[Section 9: Exports summary statistics for vars with csvs](#section9) \n",
    "<br>\n",
    "[Section 10:Export LST data from GEE](#section10) \n",
    "<br>\n",
    "[Section 11:Arcpy stats. TBD](#section11) \n",
    "<br>\n",
    "[Section 12:Package data for sharing. TBD](#section12) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b48fd5e5-75cd-442f-9da8-8f5e1e14e50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import  zipfile\n",
    "import os, math\n",
    "import geopandas as gpd\n",
    "from shutil import copyfile\n",
    "from pathlib import Path\n",
    "from shapely.geometry import Polygon\n",
    "from functools import reduce\n",
    "import plotly.express as px\n",
    "import contextily as cx\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from fiona.crs import from_epsg\n",
    "import fiona\n",
    "from random import randint\n",
    "\n",
    "import rasterio\n",
    "import rasterio.mask\n",
    "from rasterio.plot import show\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from rasterio.merge import merge\n",
    "import xarray as xr\n",
    "from rasterio.features import shapes\n",
    "\n",
    "# from rasterstats import zonal_stats\n",
    "from xrspatial.zonal import stats as zonal_stats\n",
    "from xrspatial.zonal import crosstab as zonal_crosstab\n",
    "\n",
    "from os.path import exists\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "import geemap\n",
    "import ee\n",
    "#Initialize EE\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774d6e9b",
   "metadata": {},
   "source": [
    "<a id='section0'></a>\n",
    "<h5><center> <font color='cyan'> Preface:Set paths and Then Visually Inspect AOIs</font>   </center></h5>\n",
    "\n",
    "\n",
    "***Path setup***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4be2a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aziz\\Dropbox\\CRP\\UCRA\n",
      "c:\\Users\\Aziz\\Dropbox\\CRP\\UCRA\\Bangladesh\n",
      "Benapole\n",
      "Coxs Bazar\n",
      "Jessore\n",
      "Kushtia\n",
      "Madhabdi\n",
      "Panchagarh\n",
      "Saidpur\n",
      "Mirsharai\n",
      "Feni\n",
      "Madaripur\n",
      "Natore\n",
      "Bogura\n",
      "Dinajpur\n",
      "Shariatpur\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aziz\\AppData\\Local\\Temp\\ipykernel_14300\\254904904.py:73: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  centroid = gpd.read_file(Path(city) / 'data/AOI' / (city.replace(\" \", \"_\").lower() + '_AOI.shp')).centroid\n",
      "C:\\Users\\Aziz\\AppData\\Local\\Temp\\ipykernel_14300\\254904904.py:73: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  centroid = gpd.read_file(Path(city) / 'data/AOI' / (city.replace(\" \", \"_\").lower() + '_AOI.shp')).centroid\n",
      "C:\\Users\\Aziz\\AppData\\Local\\Temp\\ipykernel_14300\\254904904.py:73: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  centroid = gpd.read_file(Path(city) / 'data/AOI' / (city.replace(\" \", \"_\").lower() + '_AOI.shp')).centroid\n",
      "C:\\Users\\Aziz\\AppData\\Local\\Temp\\ipykernel_14300\\254904904.py:73: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  centroid = gpd.read_file(Path(city) / 'data/AOI' / (city.replace(\" \", \"_\").lower() + '_AOI.shp')).centroid\n",
      "C:\\Users\\Aziz\\AppData\\Local\\Temp\\ipykernel_14300\\254904904.py:73: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  centroid = gpd.read_file(Path(city) / 'data/AOI' / (city.replace(\" \", \"_\").lower() + '_AOI.shp')).centroid\n",
      "C:\\Users\\Aziz\\AppData\\Local\\Temp\\ipykernel_14300\\254904904.py:73: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  centroid = gpd.read_file(Path(city) / 'data/AOI' / (city.replace(\" \", \"_\").lower() + '_AOI.shp')).centroid\n",
      "C:\\Users\\Aziz\\AppData\\Local\\Temp\\ipykernel_14300\\254904904.py:73: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  centroid = gpd.read_file(Path(city) / 'data/AOI' / (city.replace(\" \", \"_\").lower() + '_AOI.shp')).centroid\n",
      "C:\\Users\\Aziz\\AppData\\Local\\Temp\\ipykernel_14300\\254904904.py:73: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  centroid = gpd.read_file(Path(city) / 'data/AOI' / (city.replace(\" \", \"_\").lower() + '_AOI.shp')).centroid\n",
      "C:\\Users\\Aziz\\AppData\\Local\\Temp\\ipykernel_14300\\254904904.py:73: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  centroid = gpd.read_file(Path(city) / 'data/AOI' / (city.replace(\" \", \"_\").lower() + '_AOI.shp')).centroid\n",
      "C:\\Users\\Aziz\\AppData\\Local\\Temp\\ipykernel_14300\\254904904.py:73: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  centroid = gpd.read_file(Path(city) / 'data/AOI' / (city.replace(\" \", \"_\").lower() + '_AOI.shp')).centroid\n",
      "C:\\Users\\Aziz\\AppData\\Local\\Temp\\ipykernel_14300\\254904904.py:73: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  centroid = gpd.read_file(Path(city) / 'data/AOI' / (city.replace(\" \", \"_\").lower() + '_AOI.shp')).centroid\n",
      "C:\\Users\\Aziz\\AppData\\Local\\Temp\\ipykernel_14300\\254904904.py:73: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  centroid = gpd.read_file(Path(city) / 'data/AOI' / (city.replace(\" \", \"_\").lower() + '_AOI.shp')).centroid\n",
      "C:\\Users\\Aziz\\AppData\\Local\\Temp\\ipykernel_14300\\254904904.py:73: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  centroid = gpd.read_file(Path(city) / 'data/AOI' / (city.replace(\" \", \"_\").lower() + '_AOI.shp')).centroid\n",
      "C:\\Users\\Aziz\\AppData\\Local\\Temp\\ipykernel_14300\\254904904.py:73: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  centroid = gpd.read_file(Path(city) / 'data/AOI' / (city.replace(\" \", \"_\").lower() + '_AOI.shp')).centroid\n"
     ]
    }
   ],
   "source": [
    "parent_dir = Path(os.getcwd()).parents[0]\n",
    "print(parent_dir)\n",
    "#Set country country\n",
    "country= \"Bangladesh\"\n",
    "directory = os.path.join(parent_dir, country)\n",
    "# map_dir= os.path.join(directory, \"maps\")\n",
    "# if not os.path.exists(map_dir):\n",
    "#     os.mkdir(map_dir)\n",
    "os.chdir(directory)\n",
    "print(directory)\n",
    "\n",
    "crp_dir = Path(os.getcwd()).parents[1]\n",
    "aoi_folder = Path('data/AOI')\n",
    "output_folder = Path('data')\n",
    "\n",
    "# cities = ['Chittagong']\n",
    "cities_shapefile= ('NodalPourashavasInPolygonShariatpurFixed.shp')\n",
    "data = gpd.read_file(Path('shapefile') /cities_shapefile).to_crs(4326)\n",
    "data[\"city_name\"]=data[\"Name\"].str.replace(\"'\",\"\")\n",
    "data[\"city_name\"]=data[\"city_name\"]\n",
    "data0=data\n",
    "# Combine the clusters with indivivdual cities than then run the script on that \n",
    "cities_shapefile= ('ClustersOutline.shp')\n",
    "data1 = gpd.read_file(Path('shapefile') /cities_shapefile).to_crs(4326)\n",
    "data1[\"city_name\"]=data1.Cluster.apply(lambda x: \"cluster\"+str(x))\n",
    "data1[\"Name\"]=data1.Cluster.apply(lambda x: \"cluster\"+str(x))\n",
    "data1[\"city_name\"]=data1[\"Name\"].str.replace(\"'\",\"\")\n",
    "dflist=[ data0, data1]\n",
    "gdf = gpd.GeoDataFrame( pd.concat( dflist, ignore_index=True) )\n",
    "gdf = gdf.to_crs(4326)\n",
    "gdf.to_file(Path('shapefile') / 'compiled_clusters_plus_cities.shp' , crs = 'EPSG:4326')\n",
    "data= gdf\n",
    "\n",
    "# data[\"city_name\"]=data[\"city_name\"].str.lower()\n",
    "cities = data0[\"city_name\"].unique()\n",
    "cities=cities.tolist()\n",
    "for city in cities:\n",
    "    gdf = data.loc[data[\"city_name\"] == city]\n",
    "    gdf.to_file(Path('shapefile') / (city.replace(\" \", \"_\").lower() + '.shp'))\n",
    "    \n",
    "def create_folder(name):\n",
    "    try:\n",
    "        os.mkdir(name)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "create_folder('data') # folder for raw data\n",
    "create_folder('shapefile')\n",
    "create_folder('output') # folder for processed data\n",
    "create_folder('output/drought')\n",
    "create_folder('plots') # folder for plots\n",
    "create_folder('stats') # folder for derived statistics\n",
    "create_folder('stats/drought')\n",
    "create_folder('stats/CCKP')\n",
    "create_folder('stats/spei')\n",
    "create_folder('maps') # folder for maps\n",
    "\n",
    "# cities = ['Kinshasa', 'Kananga', 'Mbuji-Mayi', 'Lubumbashi', 'Kisangani', 'Bukavu', 'Goma', 'Tshikapa', 'Mwene-Ditu', 'Gemena', 'Gbadolite', 'Matadi', 'Kikwit', 'Bunia']\n",
    "# cities = ['Chittagong']\n",
    "for city in cities:\n",
    "    print(city)\n",
    "    create_folder(Path(city))\n",
    "    create_folder(city / Path('maps'))\n",
    "    create_folder(city / Path('data'))\n",
    "    create_folder(city / Path('data') / 'AOI')\n",
    "    create_folder(city / Path('stats'))\n",
    "for city in cities:\n",
    "    shp_4326 = gpd.read_file(Path('shapefile') / (city.replace(\" \", \"_\").lower() + '.shp')).to_crs(epsg = 4326)\n",
    "    shp_4326.to_file(Path(city) / 'data/AOI' / (city.replace(\" \", \"_\").lower() + '_AOI.shp'))\n",
    "with open('centroids.csv', 'w') as f:\n",
    "    f.write('city,x,y,utm\\n')\n",
    "    for city in cities:\n",
    "        centroid = gpd.read_file(Path(city) / 'data/AOI' / (city.replace(\" \", \"_\").lower() + '_AOI.shp')).centroid\n",
    "        # centroid = gpd.read_file(Path(city) / 'data/AOI' / (city.replace(\"'\", \"\").lower() + '_AOI.shp')).centroid\n",
    "        f.write('%s,%s,%s,%s\\n'%(city, centroid.x[0], centroid.y[0], 32600+math.ceil((centroid.x[0]+180)/6)))\n",
    "\n",
    "# all_countries = gpd.read_file((Path('data')/\"wb_countries_admin0_10m/WB_countries_Admin0_10m.shp\"))\n",
    "# country_shp = all_countries[all_countries.NAME_EN == 'Bangladesh']\n",
    "# country_shp.to_file(Path(('shapefile') / (country.lower().replace(' ', '_') + '.shp')))\n",
    "centroids = pd.read_csv('centroids.csv')\n",
    "gdf = gpd.GeoDataFrame(centroids, \n",
    "                       geometry = gpd.points_from_xy(centroids.x, centroids.y))\n",
    "\n",
    "gdf.to_file('shapefile/centroids.shp', crs = 'EPSG:4326')\n",
    "epsg_dict = dict(zip(centroids.city, centroids.utm))\n",
    "epsg_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8c56fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "STop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1a16ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "def fill_small_holes(row):\n",
    "    \"\"\"\n",
    "    This function plugs small holes.\n",
    "    Arg: \n",
    "        geodataframe \n",
    "    returns:\n",
    "        fixed geodataframe\n",
    "    \"\"\"\n",
    "    newgeom=None\n",
    "    rings = [i for i in row[\"geometry\"].interiors] #List all interior rings\n",
    "    if len(rings)>0: #If there are any rings\n",
    "        to_fill = [Polygon(ring) for ring in rings if Polygon(ring).area<sizelim] #List the ones to fill\n",
    "        if len(to_fill)>0: #If there are any to fill\n",
    "            newgeom = reduce(lambda geom1, geom2: geom1.union(geom2),[row[\"geometry\"]]+to_fill) #Union the original geometry with all holes\n",
    "    if newgeom:\n",
    "        return newgeom\n",
    "    else:\n",
    "        return row[\"geometry\"]\n",
    "def map(df):\n",
    "    df['centroid'] = df['geometry'].centroid\n",
    "    df['lat'] = df['centroid'].y\n",
    "    df['lon'] = df['centroid'].x\n",
    "    lat= df['centroid'].y\n",
    "    lon= df['centroid'].x\n",
    "    \n",
    "    fig_data_density = px.choropleth_mapbox(df,\n",
    "                           geojson=df.geometry,\n",
    "                           locations=df.index, \n",
    "                           color=df.Name, #color_continuous_scale='Reds', #scale\n",
    "                           opacity=.4,\n",
    "                           center={\"lat\": lat[0], \"lon\": lon[0]}, #mapbox_style=\"open-street-map\",\n",
    "                           zoom=6,\n",
    "                           )\n",
    "    fig_data_density.update_traces(marker_line_width=10)\n",
    "\n",
    "    s = \"https://services.arcgisonline.com/arcgis/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\"\n",
    "    fig_data_density.update_layout( \n",
    "        coloraxis_showscale=False,\n",
    "        margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0},\n",
    "        mapbox_style=\"white-bg\",\n",
    "        mapbox_layers=[\n",
    "            {\n",
    "                \"below\": \"traces\",\n",
    "                \"sourcetype\": \"raster\",\n",
    "                \"sourceattribution\": \"United States Geological Survey\",\n",
    "                \"source\": [s],\n",
    "                # \"type\": \"line\",\n",
    "                # \"color\": \"red\"\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    fig_data_density.data[0].hovertemplate = \"<span style='font-size:1.2rem; font-weight=400'>Region = %{z:,.0f}</span><br><br>\"\n",
    "    return fig_data_density\n",
    "\n",
    "def export_map(map_dir, vector_file, crs, legend_title,  visualize_column, title, map_output):\n",
    "    ax = vector_file.to_crs(crs).plot(figsize=(10, 10), \\\n",
    "                                   column= visualize_column, \\\n",
    "                                   alpha=0.6,  \\\n",
    "                                   facecolor='none',edgecolor='black' , \\\n",
    "                                   linewidth=4 , \n",
    "                                   legend=True,  \n",
    "                                   legend_kwds={'loc':'upper right', \n",
    "                                                'bbox_to_anchor':(1, 1), \n",
    "                                                'markerscale':1.01, \n",
    "                                                'title_fontsize':'small', \n",
    "                                                'fontsize':'x-small'\n",
    "                                                }  \n",
    "                                        )\n",
    "\n",
    "    cx.add_basemap(ax, source=cx.providers.Esri.WorldImagery,  crs=crs) \n",
    "    minx, miny, maxx, maxy = vector_file.total_bounds\n",
    "    ax.set_xlim(minx, maxx)\n",
    "    ax.set_ylim(miny, maxy)\n",
    "    # Legends\n",
    "    LegendElement = [\n",
    "                    Line2D([0],[0],color='black',lw=4,label=f'{legend_title}')\n",
    "                    ]\n",
    "    ax.legend(handles=LegendElement,loc='upper right')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])   \n",
    "    ax.title.set_text(f'{title}')\n",
    "    plt.tight_layout()\n",
    "    ax.figure.savefig(map_output)\n",
    "\n",
    "def export_map_of_each_city(df):\n",
    "    for index, row in df.iterrows():\n",
    "        name=row[\"Name\"]  \n",
    "        print(f\"index:{index}, name:{name}\") \n",
    "        df_filtered= df.loc[df['Name'] == name]\n",
    "        title=name.replace(\"'\" , \"\")\n",
    "        crs = 4326\n",
    "        legend_title= 'Boundary'\n",
    "        visualize_column='Name'\n",
    "        map_output= os.path.join(Path('maps') , f\"{title}_map.png\" )\n",
    "        export = export_map(Path('maps'), vector_file=df_filtered, crs=crs, legend_title=legend_title,  \\\n",
    "                        visualize_column=visualize_column, title=title, map_output=map_output)\n",
    "\n",
    "# Uncomment for visual inspection \n",
    "sizelim = 1000 #Fill holes less than 1000 m2\n",
    "# df = gpd.read_file(os.path.join(country, r\"shapefile\\NodalPourashavasInPolygonShariatpurFixed.shp\"))\n",
    "df=data\n",
    "# df['geometry'] =df['geometry']\n",
    "# df[\"geometry\"] = df.apply(fill_small_holes, axis=1)\n",
    "# export_map_of_each= export_map_of_each_city(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ca3b53",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "<h5><center> <font color='cyan'> Section 1:Urban land and built-up area data processing</font>   </center></h5>\n",
    "\n",
    "**Set paths and create centroids for the AOIs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c305601e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1 paths\n",
    "int_output_folder = Path('output/urbanland')\n",
    "# # Raw data folder. Change file path as needed\n",
    "data_folder= crp_dir.joinpath('FCS/data/urbanland')\n",
    "cities = pd.read_csv('centroids.csv').city\n",
    "centroids = pd.read_csv('centroids.csv')\n",
    "epsg_dict = dict(zip(centroids.city, centroids.utm))\n",
    "year_list = [2050, 2100]\n",
    "# SSP_list = [1, 2, 3]\n",
    "SSP_list = [ 2, 5]\n",
    "try:\n",
    "    os.mkdir(int_output_folder)\n",
    "except FileExistsError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dccbd670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop the global raster file to the country extent (with a slight buffer), so that there is no need to reproject the entire globe\n",
    "def clip_builtup_rasters(shp, data_folder, int_output_folder):\n",
    "    features = shp.geometry\n",
    "    for SSP in SSP_list:\n",
    "        for year in year_list:\n",
    "            out_file = 'ssp' + str(SSP) + '_' + str(year) + '_' + country.replace(' ', '_').lower() + '.tif'\n",
    "            if not exists(int_output_folder / out_file):\n",
    "                with rasterio.open(data_folder / ('ssp' + str(SSP) + '-geotiff') / ('ssp' + str(SSP) + '_' + str(year) + '.tif')) as src:\n",
    "                    out_image, out_transform = rasterio.mask.mask(\n",
    "                        src, features, crop=True)\n",
    "                    out_meta = src.meta.copy()\n",
    "\n",
    "                out_meta.update({\"driver\": \"GTiff\",\n",
    "                                \"height\": out_image.shape[1],\n",
    "                                \"width\": out_image.shape[2],\n",
    "                                \"transform\": out_transform})\n",
    "\n",
    "                with rasterio.open(int_output_folder / out_file, \"w\", **out_meta) as dest:\n",
    "                    dest.write(out_image)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0fe524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproject the raster files as needed and clip them to the city extents\n",
    "def clipdata_bu_proj(SSP, year, city):\n",
    "    city_no_space = city.replace(\" \", \"_\")\n",
    "    city_lower = city_no_space.lower()\n",
    "    crs = epsg_dict[city]\n",
    "    shp_name = city_no_space + '_AOI.shp'\n",
    "    shp = gpd.read_file(city / aoi_folder / shp_name).to_crs(epsg = crs)\n",
    "    features = shp.geometry\n",
    "    \n",
    "    projected_raster = 'ssp' + str(SSP) + '_' + str(year) + '_' + country.replace(' ', '_').lower() + '_' + str(crs) + '.tif'\n",
    "    unprojected_raster = 'ssp' + str(SSP) + '_' + str(year) + '_' + country.replace(' ', '_').lower() + '.tif'\n",
    "    if not exists(int_output_folder / projected_raster):\n",
    "        with rasterio.open(int_output_folder / unprojected_raster) as src:\n",
    "            dst_crs = 'EPSG:' + str(crs)\n",
    "\n",
    "            transform, width, height = calculate_default_transform(\n",
    "                src.crs, dst_crs, src.width, src.height, *src.bounds)\n",
    "            kwargs = src.meta.copy()\n",
    "            kwargs.update({\n",
    "                'crs': dst_crs,\n",
    "                'transform': transform,\n",
    "                'width': width,\n",
    "                'height': height\n",
    "            })\n",
    "\n",
    "            with rasterio.open(int_output_folder / projected_raster, 'w', **kwargs) as dst:\n",
    "                for i in range(1, src.count + 1):\n",
    "                    reproject(\n",
    "                        source=rasterio.band(src, i),\n",
    "                        destination=rasterio.band(dst, i),\n",
    "                        src_transform=src.transform,\n",
    "                        src_crs=src.crs,\n",
    "                        dst_transform=transform,\n",
    "                        dst_crs=dst_crs,\n",
    "                        resampling=Resampling.nearest)\n",
    "    \n",
    "    with rasterio.open(int_output_folder / projected_raster) as src:\n",
    "        out_image, out_transform = rasterio.mask.mask(\n",
    "            src, features, crop=True)\n",
    "        out_meta = src.meta.copy()\n",
    "        \n",
    "    out_meta.update({\"driver\": \"GTiff\",\n",
    "                     \"height\": out_image.shape[1],\n",
    "                     \"width\": out_image.shape[2],\n",
    "                     \"transform\": out_transform})\n",
    "    \n",
    "    out_file = city_lower + '_bu_ssp' + str(SSP) + \"_\" + str(year) + '.tif'\n",
    "    with rasterio.open(city / output_folder / out_file, \"w\", **out_meta) as dest:\n",
    "        dest.write(out_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b1166ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aziz\\AppData\\Local\\Temp\\ipykernel_14300\\2068849585.py:1: UserWarning: Geometry is in a geographic CRS. Results from 'buffer' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp = gpd.read_file(Path('shapefile') / (country.replace(' ', '_').lower() + '.shp')).buffer(2)\n"
     ]
    }
   ],
   "source": [
    "shp = gpd.read_file(Path('shapefile') / (country.replace(' ', '_').lower() + '.shp')).buffer(2)\n",
    "clip_builtup= clip_builtup_rasters(shp, data_folder, int_output_folder)\n",
    "for city in cities:\n",
    "    for SSP in SSP_list:\n",
    "        for year in year_list:\n",
    "            clipdata_bu_proj(SSP, year, city)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2ae98a",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "<h5><center> <font color='cyan'> Section 2: Process Sea level rise: Clip and reproject the rasters</font>   </center></h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9e4d184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data folder. Change file path as needed\n",
    "data_folder= crp_dir.joinpath('FCS/data/climatecentral')\n",
    "# create a corresponding folder on an external hard drive to store large raster files (intermediate outputs). Change file path as needed\n",
    "int_output_folder = Path(int_output_folder.parents[0]/'SLR')\n",
    "try:\n",
    "    os.mkdir(int_output_folder)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "year_list = [2050, 2100]\n",
    "slr_list = ['', '_RL10']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a92da14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get country shapefile extent\n",
    "def smart_append(element, ls):\n",
    "    if not element in ls:\n",
    "        ls.append(element)\n",
    "\n",
    "def create_lat_lon_list(shp_bounds):\n",
    "    lat_list = []\n",
    "    for i in range(len(shp_bounds)):\n",
    "        if math.floor(shp_bounds.miny[i]) >= 0:\n",
    "            hemi = 'N'\n",
    "            for y in range(math.floor(shp_bounds.miny[i]), math.ceil(shp_bounds.maxy[i])):\n",
    "                smart_append(hemi + str(y).zfill(2), lat_list)\n",
    "        elif math.ceil(shp_bounds.maxy[i]) >= 0:\n",
    "            for y in range(0, math.ceil(shp_bounds.maxy[i])):\n",
    "                smart_append('N' + str(y).zfill(2), lat_list)\n",
    "            for y in range(math.floor(shp_bounds.miny[i]), 0):\n",
    "                smart_append('S' + str(-y).zfill(2), lat_list)\n",
    "        else:\n",
    "            hemi = 'S'\n",
    "            for y in range(math.floor(shp_bounds.miny[i]), math.ceil(shp_bounds.maxy[i])):\n",
    "                smart_append(hemi + str(-y).zfill(2), lat_list)\n",
    "\n",
    "    lon_list = []\n",
    "\n",
    "    for i in range(len(shp_bounds)):\n",
    "        if math.floor(shp_bounds.minx[i]) >= 0:\n",
    "            hemi = 'E'\n",
    "            for x in range(math.floor(shp_bounds.minx[i]), math.ceil(shp_bounds.maxx[i])):\n",
    "                smart_append(hemi + str(x).zfill(3), lon_list)\n",
    "        elif math.ceil(shp_bounds.maxx[i]) >= 0:\n",
    "            for x in range(0, math.ceil(shp_bounds.maxx[i])):\n",
    "                smart_append('E' + str(x).zfill(3), lon_list)\n",
    "            for x in range(math.floor(shp_bounds.minx[i]), 0):\n",
    "                smart_append('W' + str(-x).zfill(3), lon_list)\n",
    "        else:\n",
    "            hemi = 'W'\n",
    "            for x in range(math.floor(shp_bounds.minx[i]), math.ceil(shp_bounds.maxx[i])):\n",
    "                smart_append(hemi + str(-x).zfill(3), lon_list)\n",
    "\n",
    "    return lat_list, lon_list     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy all the identified tiles into one folder\n",
    "def copy_relevent_tiffs(data_folder , int_output_folder , lat_list, lon_list):\n",
    "    for year in year_list:\n",
    "        for slr in slr_list:\n",
    "            try:\n",
    "                os.mkdir(int_output_folder / ('rcp45_50_' + str(year) + slr))\n",
    "            except FileExistsError:\n",
    "                pass\n",
    "            for lat in lat_list:\n",
    "                for lon in lon_list:\n",
    "                    # data_subfolder = 'K14U17_rcp45_50.0_' + str(year) + slr\n",
    "                    data_subfolder = 'AR6_ssp245_mediumconfidence_50.0_' + str(year) + slr\n",
    "                    if not exists(int_output_folder / ('rcp45_50_' + str(year) + slr) / (lat + lon + '.tif')):\n",
    "                        try:\n",
    "                            copyfile(data_folder / data_subfolder / (lat + lon + '.tif'),\n",
    "                                    int_output_folder / ('rcp45_50_' + str(year) + slr) / (lat + lon + '.tif'))\n",
    "                        except FileNotFoundError:\n",
    "                            pass\n",
    "\n",
    "    # merge the tiles into one raster file\n",
    "    for year in year_list:\n",
    "        for slr in slr_list:\n",
    "            raster_to_mosaic = []\n",
    "            mosaic_file = 'rcp45_50_' + str(year) + slr + '.tif'\n",
    "            \n",
    "            mosaic_list = list((int_output_folder / ('rcp45_50_' + str(year) + slr)).iterdir())\n",
    "            for p in mosaic_list:\n",
    "                raster = rasterio.open(p)\n",
    "                raster_to_mosaic.append(raster)\n",
    "            \n",
    "            mosaic, output = merge(raster_to_mosaic)\n",
    "            output_meta = raster.meta.copy()\n",
    "            output_meta.update(\n",
    "                {\"driver\": \"GTiff\",\n",
    "                    \"height\": mosaic.shape[1],\n",
    "                    \"width\": mosaic.shape[2],\n",
    "                    \"transform\": output,\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            with rasterio.open(int_output_folder / mosaic_file, 'w', **output_meta) as m:\n",
    "                m.write(mosaic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05a30905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproject the raster files as needed and clip them to the city extents\n",
    "def clipdata_slr(slr, year, city):\n",
    "    city_no_space = city.replace(\" \", \"_\")\n",
    "    city_lower = city_no_space.lower()\n",
    "    crs = epsg_dict[city]\n",
    "    shp_name = city_no_space + '_AOI.shp'\n",
    "    shp = gpd.read_file(city / aoi_folder / shp_name).to_crs(epsg = crs)\n",
    "    features = shp.geometry\n",
    "    \n",
    "    projected_raster = 'rcp45_50_' + str(year) + slr + '_' + str(crs) + '.tif'\n",
    "    unprojected_raster = 'rcp45_50_' + str(year) + slr + '.tif'\n",
    "    if not exists(int_output_folder / projected_raster):\n",
    "        with rasterio.open(int_output_folder / unprojected_raster) as src:\n",
    "            dst_crs = 'EPSG:' + str(crs)\n",
    "\n",
    "            transform, width, height = calculate_default_transform(\n",
    "                src.crs, dst_crs, src.width, src.height, *src.bounds)\n",
    "            kwargs = src.meta.copy()\n",
    "            kwargs.update({\n",
    "                'crs': dst_crs,\n",
    "                'transform': transform,\n",
    "                'width': width,\n",
    "                'height': height\n",
    "            })\n",
    "\n",
    "            with rasterio.open(int_output_folder / projected_raster, 'w', **kwargs) as dst:\n",
    "                for i in range(1, src.count + 1):\n",
    "                    reproject(\n",
    "                        source=rasterio.band(src, i),\n",
    "                        destination=rasterio.band(dst, i),\n",
    "                        src_transform=src.transform,\n",
    "                        src_crs=src.crs,\n",
    "                        dst_transform=transform,\n",
    "                        dst_crs=dst_crs,\n",
    "                        resampling=Resampling.nearest)\n",
    "    \n",
    "    try:\n",
    "        with rasterio.open(int_output_folder / projected_raster) as src:\n",
    "            out_image, out_transform = rasterio.mask.mask(\n",
    "                src, features, crop=True)\n",
    "            out_meta = src.meta.copy()\n",
    "\n",
    "        out_meta.update({\"driver\": \"GTiff\",\n",
    "                         \"height\": out_image.shape[1],\n",
    "                         \"width\": out_image.shape[2],\n",
    "                         \"transform\": out_transform})\n",
    "        \n",
    "        if np.nansum(out_image) != 0:\n",
    "            out_file = city_lower + '_slr' + slr + \"_\" + str(year) + '.tif'\n",
    "            with rasterio.open(city / output_folder / out_file, \"w\", **out_meta) as dest:\n",
    "                dest.write(out_image)\n",
    "    except ValueError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "982c9089",
   "metadata": {},
   "outputs": [],
   "source": [
    "shp_bounds = gpd.read_file(Path('shapefile') / (country.replace(' ', '_').lower() + '.shp')).bounds\n",
    "lat_list, lon_list= create_lat_lon_list(shp_bounds)\n",
    "copy_files= copy_relevent_tiffs(data_folder , int_output_folder , lat_list, lon_list  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5e0b07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in cities:\n",
    "    for slr in slr_list:\n",
    "        for year in year_list:\n",
    "            clipdata_slr(slr, year, city)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687ea5d2",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "<h5><center> <font color='cyan'> Section 3: Process CCKP Vars: Exports summary statistics</font>   </center></h5>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b13266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data folder. Change file path as needed\n",
    "data_folder= Path('data/CCKP')\n",
    "# create a corresponding folder on an external hard drive to store large raster files (intermediate outputs). Change file path as needed\n",
    "int_output_folder = Path(int_output_folder.parents[0]/'CCKP')\n",
    "try:\n",
    "    os.mkdir(int_output_folder)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cfb6a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssps = ['119', '245', '370']\n",
    "ssps = ['245', '585'] #For Bangladesh cluster analysis\n",
    "periods = ['2040-2059']\n",
    "varias0 = ['tas', 'txx', 'pr', 'r95ptot', 'cdd']\n",
    "varias1 = ['hd35', 'tr26', 'wsdi', 'r20mm', 'r50mm']\n",
    "varias = varias0 + varias1\n",
    "rps = ['20yr', '50yr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5899bf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_val = dict({s: dict({p: dict({v: {} for v in varias + ['spei12']}) for p in periods}) for s in ssps})\n",
    "ano_val = dict({s: dict({p: dict({v: {} for v in varias}) for p in periods}) for s in ssps})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c058c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing for variables without timedelta\n",
    "for ssp in ssps:\n",
    "    for period in periods:\n",
    "        for varia in varias0:\n",
    "            clim = xr.open_dataset(data_folder / ('climatology-'+varia+'-annual-mean_cmip6_annual_all-regridded-bct-ssp'+ssp+'-climatology_median_'+period+'.nc'))\n",
    "            anom = xr.open_dataset(data_folder / ('anomaly-'+varia+'-annual-mean_cmip6_annual_all-regridded-bct-ssp'+ssp+'-climatology_median_'+period+'.nc'))\n",
    "            \n",
    "            for index, row in centroids.iterrows():\n",
    "                x_coords = [row['x']-1, row['x'], row['x']+1]\n",
    "                y_coords = [row['y']-1, row['y'], row['y']+1]\n",
    "                clim_val_coords = []\n",
    "                clim_val_vals = []\n",
    "                anom_val_coords = []\n",
    "                anom_val_vals = []\n",
    "                \n",
    "                for x in x_coords:\n",
    "                    for y in y_coords:\n",
    "                        clim_val_df = clim.sel(lon = x, lat = y, method = 'nearest').to_dataframe()\n",
    "                        anom_val_df = anom.sel(lon = x, lat = y, method = 'nearest').to_dataframe()\n",
    "                        if not (clim_val_df['lon'][0], clim_val_df['lat'][0]) in clim_val_coords:\n",
    "                            clim_val_coords.append((clim_val_df['lon'][0], clim_val_df['lat'][0]))\n",
    "                            clim_val_vals.append(clim_val_df['climatology-'+varia+'-annual-mean'].mean())\n",
    "                        if not (anom_val_df['lon'][0], anom_val_df['lat'][0]) in anom_val_coords:\n",
    "                            anom_val_coords.append((anom_val_df['lon'][0], anom_val_df['lat'][0]))\n",
    "                            anom_val_vals.append(anom_val_df['anomaly-'+varia+'-annual-mean'].mean())\n",
    "                \n",
    "                clim_val = np.nanmean(clim_val_vals)\n",
    "                anom_val = np.nanmean(anom_val_vals)\n",
    "                \n",
    "                abs_val[ssp][period][varia][row['city']] = clim_val\n",
    "                ano_val[ssp][period][varia][row['city']] = anom_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d9d9b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing for variables with timedelta\n",
    "for ssp in ssps:\n",
    "    for period in periods:\n",
    "        for varia in varias1:\n",
    "            clim = xr.open_dataset(data_folder / ('climatology-'+varia+'-annual-mean_cmip6_annual_all-regridded-bct-ssp'+ssp+'-climatology_median_'+period+'.nc'))\n",
    "            anom = xr.open_dataset(data_folder / ('anomaly-'+varia+'-annual-mean_cmip6_annual_all-regridded-bct-ssp'+ssp+'-climatology_median_'+period+'.nc'))\n",
    "            \n",
    "            for index, row in centroids.iterrows():\n",
    "                x_coords = [row['x']-1, row['x'], row['x']+1]\n",
    "                y_coords = [row['y']-1, row['y'], row['y']+1]\n",
    "                clim_val_coords = []\n",
    "                clim_val_vals = []\n",
    "                anom_val_coords = []\n",
    "                anom_val_vals = []\n",
    "                \n",
    "                for x in x_coords:\n",
    "                    for y in y_coords:\n",
    "                        clim_val_df = clim.sel(lon = x, lat = y, method = 'nearest').to_dataframe()\n",
    "                        anom_val_df = anom.sel(lon = x, lat = y, method = 'nearest').to_dataframe()\n",
    "                        if not (clim_val_df['lon'][0], clim_val_df['lat'][0]) in clim_val_coords:\n",
    "                            clim_val_coords.append((clim_val_df['lon'][0], clim_val_df['lat'][0]))\n",
    "                            clim_val_vals.append(np.timedelta64(clim_val_df['climatology-'+varia+'-annual-mean'].mean()).astype('timedelta64[D]') / np.timedelta64(1, 'D'))\n",
    "                        if not (anom_val_df['lon'][0], anom_val_df['lat'][0]) in anom_val_coords:\n",
    "                            anom_val_coords.append((anom_val_df['lon'][0], anom_val_df['lat'][0]))\n",
    "                            anom_val_vals.append(np.timedelta64(anom_val_df['anomaly-'+varia+'-annual-mean'].mean()).astype('timedelta64[D]') / np.timedelta64(1, 'D'))\n",
    "                \n",
    "                clim_val = np.nanmean(clim_val_vals)\n",
    "                anom_val = np.nanmean(anom_val_vals)\n",
    "                \n",
    "                abs_val[ssp][period][varia][row['city']] = clim_val\n",
    "                ano_val[ssp][period][varia][row['city']] = anom_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1764737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing for spei12\n",
    "varia = 'spei12'\n",
    "for ssp in ssps:\n",
    "    for period in periods:\n",
    "        clim = xr.open_dataset(data_folder / ('climatology-'+varia+'-annual-mean_cmip6_annual_all-regridded-bct-ssp'+ssp+'-climatology_median_'+period+'.nc'))\n",
    "\n",
    "        for index, row in centroids.iterrows():\n",
    "                x_coords = [row['x']-1, row['x'], row['x']+1]\n",
    "                y_coords = [row['y']-1, row['y'], row['y']+1]\n",
    "                clim_val_coords = []\n",
    "                clim_val_vals = []\n",
    "                \n",
    "                for x in x_coords:\n",
    "                    for y in y_coords:\n",
    "                        clim_val_df = clim.sel(lon = x, lat = y, method = 'nearest').to_dataframe()\n",
    "                        if not (clim_val_df['lon'][0], clim_val_df['lat'][0]) in clim_val_coords:\n",
    "                            clim_val_coords.append((clim_val_df['lon'][0], clim_val_df['lat'][0]))\n",
    "                            clim_val_vals.append(clim_val_df['climatology-'+varia+'-annual-mean'].mean())\n",
    "                \n",
    "                clim_val = np.nanmean(clim_val_vals)\n",
    "                abs_val[ssp][period][varia][row['city']] = clim_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03729311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv\n",
    "for ssp in ssps:\n",
    "    for varia in varias + ['spei12']:\n",
    "        with open('stats/CCKP/clim_'+varia+'_ssp'+ssp+'.csv', 'w') as f:\n",
    "            f.write('city,'+periods[0]+'\\n')\n",
    "            for city in centroids.city:\n",
    "                f.write(\"%s,%s\\n\"%(city, abs_val[ssp][periods[0]][varia][city]))\n",
    "        if varia != 'spei12':\n",
    "            with open('stats/CCKP/anom_'+varia+'_ssp'+ssp+'.csv', 'w') as f:\n",
    "                f.write('city,'+periods[0]+'\\n')\n",
    "                for city in centroids.city:\n",
    "                    f.write(\"%s,%s\\n\"%(city, ano_val[ssp][periods[0]][varia][city]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9199537",
   "metadata": {},
   "outputs": [],
   "source": [
    "aep_val = dict({s: dict({r: {} for r in rps}) for s in ssps})\n",
    "# processing for future return period change factor\n",
    "for ssp in ssps:\n",
    "    for period in ['2035-2064']:\n",
    "        for rp in rps:\n",
    "            aep = xr.open_dataset(data_folder / ('changefactorfaep'+rp+'-rx5day-period-mean_cmip6_period_all-regridded-bct-ssp'+ssp+'-climatology_median_'+period+'.nc'))\n",
    "            \n",
    "            for index, row in centroids.iterrows():\n",
    "                x_coords = [row['x']-1, row['x'], row['x']+1]\n",
    "                y_coords = [row['y']-1, row['y'], row['y']+1]\n",
    "                aep_val_coords = []\n",
    "                aep_val_vals = []\n",
    "                \n",
    "                for x in x_coords:\n",
    "                    for y in y_coords:\n",
    "                        aep_val_df = aep.sel(lon = x, lat = y, method = 'nearest').to_dataframe()\n",
    "                        if not (aep_val_df['lon'][0], aep_val_df['lat'][0]) in aep_val_coords:\n",
    "                            aep_val_coords.append((aep_val_df['lon'][0], aep_val_df['lat'][0]))\n",
    "                            aep_val_vals.append(aep_val_df['changefactorfaep'+rp+'-rx5day-period-mean'].mean())\n",
    "                \n",
    "                aep_val1 = np.nanmean(aep_val_vals)\n",
    "                \n",
    "                aep_val[ssp][rp][row['city']] = aep_val1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71dbe775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to csv\n",
    "for ssp in ssps:\n",
    "    for rp in rps:\n",
    "        with open('stats/CCKP/aep_'+rp+'_ssp'+ssp+'.csv', 'w') as f:\n",
    "            f.write('city,'+'2035-2064'+'\\n')\n",
    "            for city in centroids.city:\n",
    "                f.write(\"%s,%s\\n\"%(city, aep_val[ssp][rp][city]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b5501b",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "<h5><center> <font color='cyan'> Section 4: Process  historical SPEI data: Exports summary statistics</font>   </center></h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5901d129",
   "metadata": {},
   "source": [
    "***Set Data Path and years configs***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e73e9bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data folder. Change file path as needed\n",
    "data_folder= Path('data/SPEI')\n",
    "periods = ['01', '12', '48']\n",
    "years = range(2011, 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b3ac4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "spei_val = dict({p: dict({c: {} for c in centroids.city}) for p in periods})\n",
    "for period in periods:\n",
    "    spei_nc = xr.open_dataset(data_folder / ('spei'+period+'.nc'))\n",
    "    for index, row in centroids.iterrows():\n",
    "        for year in years:\n",
    "            for month in range(1, 13):\n",
    "                time1 = str(year) + '-' + str(month) + '-15'\n",
    "                val = spei_nc.sel(lon = row['x'], lat = row['y'], time = time1, method = 'nearest')['spei'].to_dict()['data']\n",
    "                spei_val[period][row['city']][time1] = val\n",
    "# write to csv\n",
    "for period in periods:\n",
    "    with open('stats/spei/spei'+period+'.csv', 'w') as f:\n",
    "        f.write('city,date,spei\\n')\n",
    "        for city in centroids.city:\n",
    "            for year in years:\n",
    "                for month in range(1, 13):\n",
    "                    time1 = str(year) + '-' + str(month) + '-15'\n",
    "                    f.write('%s,%s,%s\\n' % (city, time1, spei_val[period][city][time1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53796e79",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "<h5><center> <font color='cyan'> Section 5: Process PM2.5 data : Exports summary statistics</font>   </center></h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb76cf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = Path('data\\Global Annual PM2.5 Grids 1998-2019')\n",
    "def unzip_files(data_folder):\n",
    "    extension = \".zip\"\n",
    "    os.chdir(data_folder) # change directory from working dir to dir with files\n",
    "    for item in os.listdir(data_folder): # loop through items in dir\n",
    "        if item.endswith(extension): # check for \".zip\" extension\n",
    "            file_name = os.path.abspath(item) # get full path of files\n",
    "            zip_ref = zipfile.ZipFile(file_name) # create zipfile object\n",
    "            zip_ref.extractall(data_folder) # extract file to dir\n",
    "            zip_ref.close() # close file\n",
    "            os.remove(file_name) # delete zipped file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8bf0b3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipdata_air(city, year):\n",
    "    city_no_space = city.replace(\" \", \"_\")\n",
    "    city_lower = city_no_space.lower()\n",
    "    file = city / aoi_folder / (city_lower + '_AOI.shp')\n",
    "    \n",
    "    with fiona.open(file, \"r\") as shapefile:\n",
    "        features = [feature[\"geometry\"] for feature in shapefile]\n",
    "        \n",
    "        # Raw data folder. Change file path as needed\n",
    "        # input_raster = \"F:/World Bank/City Scan/data/Global Annual PM2.5 Grids 1998-2019/sdei-global-annual-gwr-pm2-5-modis-misr-seawifs-aod-v4-gl-03-\" + str(year) + \".tif\"\n",
    "        input_raster = f\"{data_folder}/sdei-global-annual-gwr-pm2-5-modis-misr-seawifs-aod-v4-gl-03-\" + str(year) + \".tif\"\n",
    "        with rasterio.open(input_raster) as src:\n",
    "            # shapely presumes all operations on two or more features exist in the same Cartesian plane.\n",
    "            out_image, out_transform = rasterio.mask.mask(\n",
    "                src, features, crop=True)\n",
    "            out_meta = src.meta.copy()\n",
    "\n",
    "        out_meta.update({\"driver\": \"GTiff\",\n",
    "                         \"height\": out_image.shape[1],\n",
    "                         \"width\": out_image.shape[2],\n",
    "                         \"transform\": out_transform})\n",
    "\n",
    "        output_file = city_lower + '_air_quality_' + str(year) + '.tif'\n",
    "        with rasterio.open(Path(city) / output_folder / output_file, \"w\", **out_meta) as dest:\n",
    "            dest.write(out_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee98bcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for city in cities:\n",
    "    for year in range(1998, 2020):\n",
    "        clipdata_air(city, year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92de90a7",
   "metadata": {},
   "source": [
    "<a id='section6'></a>\n",
    "<h5><center> <font color='cyan'> Section 6: Process heat increase due to urban land expansion : Exports summary statistics</font>   </center></h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b60d045b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = Path('data/Heat increase due to urban land expansion')\n",
    "shp_folder = Path('data/AOI')\n",
    "\n",
    "def reproj_heat(input_raster):\n",
    "    filename = input_raster + '.tif'\n",
    "    outfile = input_raster + '_4326.tif'\n",
    "    with rasterio.open(data_folder / filename) as src:\n",
    "        dst_crs = 'EPSG:4326'\n",
    "        \n",
    "        transform, width, height = calculate_default_transform(\n",
    "            src.crs, dst_crs, src.width, src.height, *src.bounds)\n",
    "        kwargs = src.meta.copy()\n",
    "        kwargs.update({\n",
    "            'crs': dst_crs,\n",
    "            'transform': transform,\n",
    "            'width': width,\n",
    "            'height': height\n",
    "        })\n",
    "\n",
    "        with rasterio.open(data_folder / outfile, 'w', **kwargs) as dst:\n",
    "            for i in range(1, src.count + 1):\n",
    "                reproject(\n",
    "                    source=rasterio.band(src, i),\n",
    "                    destination=rasterio.band(dst, i),\n",
    "                    src_transform=src.transform,\n",
    "                    src_crs=src.crs,\n",
    "                    dst_transform=transform,\n",
    "                    dst_crs=dst_crs,\n",
    "                    resampling=Resampling.nearest)\n",
    "\n",
    "def clip_heat(input_raster):\n",
    "    input_raster_name = input_raster + '_4326.tif'\n",
    "    with rasterio.open(data_folder / input_raster_name) as src:\n",
    "        out_image, out_transform = rasterio.mask.mask(\n",
    "            src, features, crop=True)\n",
    "        out_meta = src.meta.copy()\n",
    "        \n",
    "    out_meta.update({\"driver\": \"GTiff\",\n",
    "                     \"height\": out_image.shape[1],\n",
    "                     \"width\": out_image.shape[2],\n",
    "                     \"transform\": out_transform})\n",
    "    \n",
    "    out_file = city.replace(' ', '_').lower() + '_' + input_raster + '.tif'\n",
    "    with rasterio.open(city / output_folder / out_file, \"w\", **out_meta) as dest:\n",
    "        dest.write(out_image)\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c050c9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raster_list = ['urban-ssp1_day_sum', 'urban-ssp1_nig_sum',\n",
    "#                'urban-ssp2_day_sum', 'urban-ssp2_nig_sum',\n",
    "#                'urban-ssp3_day_sum', 'urban-ssp3_nig_sum']\n",
    "raster_list = ['urban-ssp2_day_sum', 'urban-ssp2_nig_sum',\n",
    "               'urban-ssp5_day_sum', 'urban-ssp5_nig_sum']\n",
    "for city in cities:\n",
    "    aoi_name = city.replace(' ', '_').lower() + '_AOI.shp'\n",
    "    shp = gpd.read_file(city/shp_folder/aoi_name) #.buffer(0.01)\n",
    "    features = shp.geometry\n",
    "    for raster in raster_list:\n",
    "        reproj_heat(raster)\n",
    "        clip_heat(raster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6d9684",
   "metadata": {},
   "source": [
    "<a id='section7'></a>\n",
    "<h5><center> <font color='cyan'> Section 7: Process Landslide Data : Exports summary statistics</font>   </center></h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reproj_ls(input_folder, input_raster):\n",
    "    filename = input_raster + '.tif'\n",
    "    outfile = input_raster + '_4326.tif'\n",
    "    with rasterio.open(input_folder/filename) as src:\n",
    "        dst_crs = 'EPSG:4326'\n",
    "        transform, width, height = calculate_default_transform(\n",
    "            src.crs, dst_crs, src.width, src.height, *src.bounds)\n",
    "        kwargs = src.meta.copy()\n",
    "        kwargs.update({\n",
    "            'crs': dst_crs,\n",
    "            'transform': transform,\n",
    "            'width': width,\n",
    "            'height': height\n",
    "        })\n",
    "\n",
    "        with rasterio.open(input_folder / outfile, 'w', **kwargs) as dst:\n",
    "            for i in range(1, src.count + 1):\n",
    "                reproject(\n",
    "                    source=rasterio.band(src, i),\n",
    "                    destination=rasterio.band(dst, i),\n",
    "                    src_transform=src.transform,\n",
    "                    src_crs=src.crs,\n",
    "                    dst_transform=transform,\n",
    "                    dst_crs=dst_crs,\n",
    "                    resampling=Resampling.nearest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8dc75d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipdata_ls(city):\n",
    "    city_no_space = city.replace(\" \", \"_\")\n",
    "    city_lower = city_no_space.lower()\n",
    "    file = city / aoi_folder / (city_lower + '_AOI.shp')\n",
    "    print('1')\n",
    "    \n",
    "    with fiona.open(file, \"r\") as shapefile:\n",
    "        features = [feature[\"geometry\"] for feature in shapefile]\n",
    "        # Raw data folder. Change file path as needed\n",
    "        # # input_raster = r'F:\\World Bank\\City Scan\\data\\Landslide\\Global landslide hazard map - Rainfall trigger (1980-2018, median) - COG.tif'\n",
    "        print('2')\n",
    "        input_raster = Path(data_folder/'LS_RF_Median_1980_2018_COG_4326.tif')\n",
    "        with rasterio.open(input_raster) as src:\n",
    "            # shapely presumes all operations on two or more features exist in the same Cartesian plane.\n",
    "            out_image, out_transform = rasterio.mask.mask(\n",
    "                src, features, crop=True)\n",
    "            out_meta = src.meta.copy()\n",
    "            print('3')\n",
    "\n",
    "        out_meta.update({\"driver\": \"GTiff\",\n",
    "                         \"height\": out_image.shape[1],\n",
    "                         \"width\": out_image.shape[2],\n",
    "                         \"transform\": out_transform,\n",
    "                         'nodata': 0})\n",
    "        \n",
    "        print('4')\n",
    "        if np.nansum(out_image) != 0:\n",
    "            output_file = city_lower + '_landslide.tif'\n",
    "            print('5')\n",
    "            with rasterio.open(Path(city) / output_folder / output_file, \"w\", **out_meta) as dest:\n",
    "                dest.write(out_image)\n",
    "                print(f'Wrote {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a7afc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "Wrote coxs_bazar_landslide.tif\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "CPU times: total: 57.1 s\n",
      "Wall time: 8min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_raster= \"LS_RF_Median_1980_2018_COG\"\n",
    "data_folder = Path('data/Landslide')\n",
    "reproj = reproj_ls(data_folder, input_raster)\n",
    "for city in cities:\n",
    "    clipdata_ls(city)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f21aa9",
   "metadata": {},
   "source": [
    "<a id='section7_1'></a>\n",
    "<h5><center> <font color='cyan'> Section 7.1: Processes historical and current built-up data from World Settlement Footprint</font>   </center></h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4334f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the WSF tiles that overlap with the AOIs\n",
    "def download_wsf(city, wsf_type):\n",
    "    # data_folder = Path(r'F:\\World Bank\\City Scan') / country / ('data/WSF' + wsf_type)  # change file path as needed\n",
    "    data_folder = Path('data/WSF' + wsf_type)  # change file path as needed\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(data_folder)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    city_no_space = city.replace(\" \", \"_\")\n",
    "    city_lower = city_no_space.lower()\n",
    "    shp_name = city_no_space + '_AOI.shp'\n",
    "    shp = gpd.read_file(city / aoi_folder / shp_name)\n",
    "    shp_bounds = shp.bounds\n",
    "    \n",
    "    for i in range(len(shp_bounds)):\n",
    "        for x in range(math.floor(shp_bounds.minx[i] - shp_bounds.minx[i] % 2), math.ceil(shp_bounds.maxx[i]), 2):\n",
    "            for y in range(math.floor(shp_bounds.miny[i] - shp_bounds.miny[i] % 2), math.ceil(shp_bounds.maxy[i]), 2):\n",
    "                file_name = 'WSF' + wsf_type + '_v1_' + str(x) + '_' + str(y)\n",
    "                if not exists(data_folder / (file_name + '.tif')):\n",
    "                    if wsf_type == 'evolution':\n",
    "                        file = requests.get('https://download.geoservice.dlr.de/WSF_EVO/files/' + file_name + '/' + file_name + '.tif')\n",
    "                    elif wsf_type == '2019':\n",
    "                        file = requests.get('https://download.geoservice.dlr.de/WSF2019/files/' + file_name + '.tif')\n",
    "                    open(data_folder / (file_name + '.tif'), 'wb').write(file.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8fd36b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def download_wsf():\n",
    "epsg_dict = dict(zip(centroids.city, centroids.utm))\n",
    "wsf_types = ['evolution', '2019']\n",
    "for wsf_type in wsf_types:\n",
    "    for city in cities:\n",
    "        download_wsf(city, wsf_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_the_tiles():\n",
    "    # merge the WSF tiles into one raster file\n",
    "    for wsf_type in wsf_types:\n",
    "        try:\n",
    "            raster_to_mosaic = []\n",
    "            # mosaic_file = 'WSF' + wsf_type + '.tif'\n",
    "            mosaic_file = 'WSF_mosaic' + wsf_type + '.tif'\n",
    "            remove_mosaic_file= Path('data/WSF' + wsf_type)/ mosaic_file\n",
    "            if os.path.isfile(remove_mosaic_file):\n",
    "                os.remove(remove_mosaic_file)\n",
    "            else:\n",
    "                # If it fails, inform the user.\n",
    "                print(\"Not deleted bcz: %s file not found\" % remove_mosaic_file)\n",
    "\n",
    "            # mosaic_list = list((Path(r'F:\\World Bank\\City Scan') / country / 'data' / ('WSF' + wsf_type)).iterdir())\n",
    "            mosaic_list = list((Path('data/WSF' + wsf_type)).iterdir())\n",
    "\n",
    "            for p in mosaic_list:\n",
    "                raster = rasterio.open(p)\n",
    "                raster_to_mosaic.append(raster)\n",
    "\n",
    "            mosaic, output = merge(raster_to_mosaic)\n",
    "            output_meta = raster.meta.copy()\n",
    "            output_meta.update(\n",
    "                {\"driver\": \"GTiff\",\n",
    "                    \"height\": mosaic.shape[1],\n",
    "                    \"width\": mosaic.shape[2],\n",
    "                    \"transform\": output,\n",
    "                }\n",
    "            )\n",
    "    \n",
    "            # with rasterio.open(Path(r'F:\\World Bank\\City Scan') / country / 'data' / ('WSF' + wsf_type) / mosaic_file, 'w', **output_meta) as m:\n",
    "            with rasterio.open(Path('data/WSF' + wsf_type)/ mosaic_file, 'w', **output_meta) as m:\n",
    "                m.write(mosaic)\n",
    "        except MemoryError:\n",
    "            print(wsf_type)\n",
    "            print('MemoryError. Try GIS instead for merging.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipdata_wsf(city, wsf_type):\n",
    "    # data_folder = Path(r'F:\\World Bank\\City Scan') / country / ('data/WSF' + wsf_type)\n",
    "    # data_folder = Path('data/WSF' + wsf_type)\n",
    "    data_folder = Path('data/WSF' + wsf_type)\n",
    "\n",
    "    \n",
    "    city_no_space = city.replace(\" \", \"_\")\n",
    "    city_lower = city_no_space.lower()\n",
    "    shp_name = city_no_space + '_AOI.shp'\n",
    "    shp = gpd.read_file(city / aoi_folder / shp_name)\n",
    "    features = shp.geometry\n",
    "    \n",
    "    # input_raster = data_folder / (\"WSF\" + wsf_type + \".tif\")\n",
    "    input_raster = data_folder / (\"WSF_mosaic\" + wsf_type + \".tif\")\n",
    "    with rasterio.open(input_raster) as src:\n",
    "        out_image, out_transform = rasterio.mask.mask(\n",
    "            src, features, crop=True)\n",
    "        out_meta = src.meta.copy()\n",
    "\n",
    "        out_meta.update({\"driver\": \"GTiff\",\n",
    "                         \"height\": out_image.shape[1],\n",
    "                         \"width\": out_image.shape[2],\n",
    "                         \"transform\": out_transform})\n",
    "\n",
    "        output_4326_raster_clipped = city_lower + \"_WSF\" + wsf_type + \"_4326.tif\"\n",
    "\n",
    "        with rasterio.open(city / output_folder / output_4326_raster_clipped, \"w\", **out_meta) as dest:\n",
    "            dest.write(out_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7a41e84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def utm_wsf(city, wsf_type):\n",
    "    city_no_space = city.replace(\" \", \"_\")\n",
    "    city_lower = city_no_space.lower()\n",
    "    shp_name = city_no_space + '_AOI.shp'\n",
    "    crs = epsg_dict.get(city)\n",
    "    shp = gpd.read_file(city / aoi_folder / shp_name).to_crs(epsg = crs)\n",
    "    features = shp.geometry\n",
    "    \n",
    "    with rasterio.open(city / output_folder / (city_lower + \"_WSF\" + wsf_type + \"_4326.tif\")) as src:\n",
    "        dst_crs = 'EPSG:' + str(crs)\n",
    "\n",
    "        transform, width, height = calculate_default_transform(\n",
    "            src.crs, dst_crs, src.width, src.height, *src.bounds)\n",
    "        kwargs = src.meta.copy()\n",
    "        kwargs.update({\n",
    "            'crs': dst_crs,\n",
    "            'transform': transform,\n",
    "            'width': width,\n",
    "            'height': height\n",
    "        })\n",
    "\n",
    "        with rasterio.open(city / output_folder / (city_lower + '_WSF' + wsf_type + '_utm.tif'), 'w', **kwargs) as dst:\n",
    "            for i in range(1, src.count + 1):\n",
    "                reproject(\n",
    "                    source=rasterio.band(src, i),\n",
    "                    destination=rasterio.band(dst, i),\n",
    "                    src_transform=src.transform,\n",
    "                    src_crs=src.crs,\n",
    "                    dst_transform=transform,\n",
    "                    dst_crs=dst_crs,\n",
    "                    resampling=Resampling.nearest)\n",
    "    \n",
    "    if wsf_type == 'evolution':\n",
    "        with rasterio.open(city / output_folder / (city_lower + '_WSF' + wsf_type + '_utm.tif')) as src:\n",
    "            out_image = src.read()\n",
    "            pixelSizeX, pixelSizeY = src.res\n",
    "\n",
    "        year_dict = {}\n",
    "        for year in range(1985, 2016):\n",
    "            if year == 1985:\n",
    "                year_dict[year] = np.count_nonzero(\n",
    "                out_image == year) * pixelSizeX * pixelSizeY / 1000000\n",
    "            else:\n",
    "                year_dict[year] = np.count_nonzero(\n",
    "                out_image == year) * pixelSizeX * pixelSizeY / 1000000 + year_dict[year-1]\n",
    "\n",
    "        # save CSV\n",
    "        with open(city / output_folder / (city_lower + \"_built_up_stats.csv\"), 'w') as f:\n",
    "            f.write(\"year,cumulative sq km\\n\")\n",
    "            for key in year_dict.keys():\n",
    "                f.write(\"%s,%s\\n\" % (key, year_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reclass_wsf(city, wsf_type = 'evolution'):\n",
    "    city_no_space = city.replace(\" \", \"_\")\n",
    "    city_lower = city_no_space.lower()\n",
    "    \n",
    "    with rasterio.open(city / output_folder / (city_lower + '_WSF' + wsf_type + '_4326.tif')) as src:\n",
    "        out_image = src.read()\n",
    "        out_meta = src.meta.copy()\n",
    "    \n",
    "    out_image[0][out_image[0] < 1985] = 0\n",
    "    out_image[0][(out_image[0] <= 2015) & (out_image[0] >= 2006)] = 4\n",
    "    out_image[0][(out_image[0] < 2006) & (out_image[0] >= 1996)] = 3\n",
    "    out_image[0][(out_image[0] < 1996) & (out_image[0] >= 1986)] = 2\n",
    "    out_image[0][out_image[0] == 1985] = 1\n",
    "    \n",
    "    out_file = city_lower + '_WSF' + wsf_type + '_reclass.tif'\n",
    "    with rasterio.open(city / output_folder / out_file, \"w\", **out_meta) as dest:\n",
    "        dest.write(out_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polygonize_wsf(city, wsf_type = '2019'):\n",
    "    city_no_space = city.replace(\" \", \"_\")\n",
    "    city_lower = city_no_space.lower()\n",
    "    \n",
    "    mask = None\n",
    "    \n",
    "    with rasterio.open(city / output_folder / (city_lower + '_WSF' + wsf_type + '_4326.tif')) as src:\n",
    "        image = src.read(1)\n",
    "        results = ({'properties': {'raster_val': v}, 'geometry': s} for i, (s, v) in enumerate(shapes(image, mask=mask, transform=src.transform)))\n",
    "        geoms = list(results)\n",
    "        gpd_polygonized_raster = gpd.GeoDataFrame.from_features(geoms)\n",
    "        gpd_polygonized_raster = gpd_polygonized_raster[gpd_polygonized_raster.raster_val != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eed1ee",
   "metadata": {},
   "source": [
    "**Loop for running the wsf functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1c68e4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: data\\WSFevolution\\WSF_mosaicevolution.tif file not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: data\\WSF2019\\WSF_mosaic2019.tif file not found\n"
     ]
    }
   ],
   "source": [
    "def clip_and_reclass():\n",
    "    # download_wsf(city, wsf_type)\n",
    "    merge_the_tiles()\n",
    "    for wsf_type in wsf_types:\n",
    "        for city in cities:\n",
    "            clipdata_wsf(city, wsf_type)\n",
    "            if wsf_type == 'evolution':\n",
    "                utm_wsf(city, wsf_type)\n",
    "                reclass_wsf(city)\n",
    "            if wsf_type == '2019':\n",
    "                polygonize_wsf(city)\n",
    "                \n",
    "clip_and_reclass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a2ab76",
   "metadata": {},
   "source": [
    "<a id='section8'></a>\n",
    "<h5><center> <font color='cyan'> Section 8: Process drought data : Exports summary statistics</font>   </center></h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c89c3b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = Path('data/drought')\n",
    "def unzip_files(data_folder):\n",
    "    extension = \".zip\"\n",
    "    os.chdir(data_folder) # change directory from working dir to dir with files\n",
    "    for item in os.listdir(data_folder): # loop through items in dir\n",
    "        if item.endswith(extension): # check for \".zip\" extension\n",
    "            file_name = os.path.abspath(item) # get full path of files\n",
    "            zip_ref = zipfile.ZipFile(file_name) # create zipfile object\n",
    "            zip_ref.extractall(data_folder) # extract file to dir\n",
    "            zip_ref.close() # close file\n",
    "            os.remove(file_name) # delete zipped file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aziz\\AppData\\Local\\Temp\\ipykernel_14300\\1189311192.py:1: UserWarning: Geometry is in a geographic CRS. Results from 'buffer' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp = gpd.read_file(Path('shapefile') / (country.replace(' ', '_').lower() + '.shp')).buffer(50)\n"
     ]
    }
   ],
   "source": [
    "shp = gpd.read_file(Path('shapefile') / (country.replace(' ', '_').lower() + '.shp')).buffer(50)\n",
    "features = shp.geometry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "da49f600",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_list = {#'rdria': 't',\n",
    "               #'spg01': 'm',\n",
    "               #'spg12': 'm',\n",
    "               'twsan': 'm'}\n",
    "month_list = ['01', '02', '03', '04', '05', '06',\n",
    "              '07', '08', '09', '10', '11', '12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_drought(folder, raster_prefix, raster_suffix, year, date):    \n",
    "    try:\n",
    "        input_raster= folder /  (raster_prefix + '_m_wld_' + year + date + '01_' + raster_suffix + '.tif')\n",
    "        # with rasterio.open(folder + '\\\\' + raster_prefix + '_m_wld_' + year + date + '01_' + raster_suffix + '.tif') as src:\n",
    "        with rasterio.open(input_raster) as src:\n",
    "            out_image, out_transform = rasterio.mask.mask(\n",
    "                src, features, crop=True)\n",
    "            out_meta = src.meta.copy()\n",
    "            # print(f'input_raster : {input_raster}')\n",
    "        out_meta.update({\"driver\": \"GTiff\",\n",
    "                         \"height\": out_image.shape[1],\n",
    "                         \"width\": out_image.shape[2],\n",
    "                         \"transform\": out_transform})\n",
    "        out_raster = Path('output/drought') / (raster_prefix + '_' + year + date + \".tif\")\n",
    "        # with rasterio.open('output/drought/' + raster_prefix + '_' + year + date + \".tif\", \"w\", **out_meta) as dest:\n",
    "        with rasterio.open(out_raster, \"w\", **out_meta) as dest:\n",
    "            dest.write(out_image)\n",
    "            # print(f'out_raster: {out_raster}')\n",
    "\n",
    "    \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "464ffe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for raster in raster_list.keys():\n",
    "    for year in range(2011, 2021):\n",
    "        for month in month_list:\n",
    "            clip_drought(data_folder, raster, raster_list.get(raster), str(year), str(month))\n",
    "            # clip_drought(folder_list[3], raster, raster_list.get(raster), str(year), str(month))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2576bd1d",
   "metadata": {},
   "source": [
    "<a id='section9'></a>\n",
    "<h5><center> <font color='cyan'> Section 9: Exports summary statistics for vars with csvs</font>   </center></h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bde693",
   "metadata": {},
   "source": [
    "**AVERAGE TEMPERATURE**\n",
    "\n",
    "**The following snippets copies the GEE LST files to cities data dirs and then exports the average temperature csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a0775b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_gee_output(city, raster_name):\n",
    "    with rasterio.open(Path('output') / 'GEE' / (raster_name + '.tif')) as src:\n",
    "        array = src.read(1)\n",
    "        out_meta = src.meta.copy()\n",
    "        out_meta.update({'nodata': 'nan'})\n",
    "    with rasterio.open(city / Path('data') / (raster_name + '.tif'), 'w', **out_meta) as dest:\n",
    "        dest.write(array, 1)\n",
    "        \n",
    "for city in cities:\n",
    "    crop_gee_output(city, city.replace(\" \", \"_\").lower() + '_Summer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_temp(city):\n",
    "    city_nospace = city.replace(\" \", \"_\").lower()\n",
    "    temp_file = city / Path('data') / (city_nospace + '_Summer.tif')\n",
    "    # temp_file = city / Path('data') / (city_nospace + '_urban-ssp2_day_sum.tif')\n",
    "    temp = rasterio.open(temp_file)\n",
    "    temp_array = temp.read(1)\n",
    "    # print(temp_array)\n",
    "    return np.nanmean(temp_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_avg_temp = {}\n",
    "for city in cities:\n",
    "    cities_avg_temp[city] = avg_temp(city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5038298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stats/avg_temp.csv', 'w') as f:\n",
    "    f.write('city,avg\\n')\n",
    "    for city in cities_avg_temp.keys():\n",
    "        f.write(\"%s,%s\\n\"%(city, cities_avg_temp[city]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a585ee",
   "metadata": {},
   "source": [
    "**AIR QUALITY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6b1e3cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_air(city, year):\n",
    "    city_nospace = city.replace(\" \", \"_\").lower()\n",
    "    temp_file = city / Path('data') / (city_nospace + '_air_quality_' + str(year) + '.tif')\n",
    "    temp = rasterio.open(temp_file)\n",
    "    temp_array = temp.read(1)\n",
    "    temp_array = temp_array[temp_array >= 0]\n",
    "    return np.nanmean(temp_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6f294254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bad_air(city, threshold = 5):  # threshold for the definition of \"bad air\"; default is 5 ug/m3\n",
    "    city_nospace = city.replace(\" \", \"_\").lower()\n",
    "    air_file = city / Path('data') / (city_nospace + '_air_quality_' + str(year) + '.tif')\n",
    "    air = rasterio.open(air_file)\n",
    "    air_array = air.read(1)\n",
    "    # print(air_array)\n",
    "    return sum(sum(air_array >= threshold)) / sum(sum(air_array != air.meta['nodata']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "23e30764",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_avg_air = {}\n",
    "for city in cities:\n",
    "    cities_avg_air[city] = {}\n",
    "    for year in range(1998, 2020):\n",
    "        cities_avg_air[city][year] = avg_air(city, year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9ac5c3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_bad_air = {}\n",
    "for city in cities:\n",
    "    cities_bad_air[city] = {}\n",
    "    for year in range(1998, 2020):\n",
    "        cities_bad_air[city][year] = bad_air(city)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5d7ac741",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stats/avg_air_1998_2019.csv', 'w') as f:\n",
    "    f.write('city,year,avg,pct_bad_air\\n')\n",
    "    for city in cities_avg_air.keys():\n",
    "        for year in range(1998, 2020):\n",
    "            f.write(\"%s,%s,%s,%s\\n\"%(city, year, cities_avg_air[city][year], cities_bad_air[city][year]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fb8d73",
   "metadata": {},
   "source": [
    " **LANDSLIDE AVERAGE FREQUENCY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0a388afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def landslide(city):\n",
    "    city_nospace = city.replace(\" \", \"_\").lower()\n",
    "    ls_file = city / Path('data') / (city_nospace + '_landslide.tif')\n",
    "    ls = rasterio.open(ls_file)\n",
    "    ls_array = ls.read(1)\n",
    "    return np.nanmean(ls_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9ddf4755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped Benapole\n",
      "Skipped Jessore\n",
      "Skipped Kushtia\n",
      "Skipped Madhabdi\n",
      "Skipped Panchagarh\n",
      "Skipped Saidpur\n",
      "Skipped Mirsharai\n",
      "Skipped Feni\n",
      "Skipped Madaripur\n",
      "Skipped Natore\n",
      "Skipped Bogura\n",
      "Skipped Dinajpur\n",
      "Skipped Shariatpur\n"
     ]
    }
   ],
   "source": [
    "cities_ls = {}\n",
    "for city in cities:\n",
    "    try:\n",
    "        landslide_value = landslide(city)\n",
    "        cities_ls[city] = landslide_value\n",
    "    except:\n",
    "        print(f\"Skipped {city}\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "93722e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stats/landslide_avg.csv', 'w') as f:\n",
    "    f.write('city,avg\\n')\n",
    "    for city in cities_ls.keys():\n",
    "        f.write(\"%s,%s\\n\"%(city, cities_ls[city]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5b0515",
   "metadata": {},
   "source": [
    "<a id='section10'></a>\n",
    "<h5><center> <font color='cyan'> Section 10: Export LST data from GEE. Ran this on Colab for this round.TBD</font>   </center></h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ed54a7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_landsurface_temperature( country , cities_reprojected, max_value , years, month0 ,  month1):\n",
    "\n",
    "    \"\"\"\n",
    "    export_landsurface_temperature: Exports Land Surface temperature rasters for a given country in the cities shapefile\n",
    "\n",
    "    :param country: Name of the country\n",
    "    :param cities_reprojected: geopandas read cities shapefile\n",
    "    :param max_value: cap the number of cities\n",
    "    :param years: years\n",
    "    :param month0 : first hottest month\n",
    "    :param month1 : last hottest month\n",
    "\n",
    "    :return: confirms the export of rasters\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a jsondictionary from the geometry of the  shapefile\n",
    "    geom = cities_reprojected['geometry']\n",
    "    jsonDict = eval(geom.to_json())\n",
    "\n",
    "    for index, row in cities_reprojected.iterrows():\n",
    "        try:\n",
    "          city= row['NAME_1'].lower() #instruct the column for city name in the city shapefile\n",
    "          if index  <= max_value:\n",
    "              city_number= index\n",
    "              for x in jsonDict['features'][city_number]['geometry']['coordinates']:\n",
    "                  AOI = ee.Geometry.Polygon(x) #cast polygon as ee geometry\n",
    "                  landsat = ee.ImageCollection(\"LANDSAT/LC08/C02/T1_L2\")\n",
    "                  print(1)\n",
    "                  # Filter for hottest months in the past X years\n",
    "                  def filter_hot_month(i):\n",
    "                      return ee.Filter.date(years[i] + '-' + month0 + '-01', years[i] + '-' + month1 + '-01')\n",
    "\n",
    "                  range_list= map(filter_hot_month, list(range(0, 10))) #combination of months and years\n",
    "                #   range_list= list(map(filter_hot_month, list(range(0, 10)))) #combination of months and years\n",
    "                  print(1.2)\n",
    "                  rangefilter = ee.Filter.Or(range_list)\n",
    "                  print(2)\n",
    "\n",
    "                  # Define a function to scale the data and mask unwanted pixels\n",
    "                  def maskL457sr(image):\n",
    "                      # Bit 0 - Fill\n",
    "                      # Bit 1 - Dilated Cloud\n",
    "                      # Bit 2 - Cirrus (high confidence)\n",
    "                      # Bit 3 - Cloud\n",
    "                      # Bit 4 - Cloud Shadow\n",
    "                      qaMask = image.select('QA_PIXEL').bitwiseAnd(int('11111', 2)).eq(0)\n",
    "                      saturationMask = image.select('QA_RADSAT').eq(0)\n",
    "                      # Apply the scaling factors to the appropriate bands.\n",
    "                      thermalBand = image.select('ST_B10').multiply(0.00341802).add(149.0)\n",
    "                      # Replace the original bands with the scaled ones and apply the masks.\n",
    "                      return image.addBands(thermalBand, None, True).updateMask(qaMask).updateMask(saturationMask)\n",
    "                  print(3)\n",
    "\n",
    "                  # Apply filter and mask\n",
    "                  collectionSummer = landsat.filter(rangefilter).filterBounds(AOI).map(maskL457sr).select('ST_B10').mean().add(-273.15).clip(AOI)\n",
    "                  print(4)\n",
    "\n",
    "                  task = ee.batch.Export.image.toDrive(**{\n",
    "                      'image': collectionSummer,\n",
    "                      'description': city + \"_Summer\",\n",
    "                      'folder': country,\n",
    "                      'region': AOI,\n",
    "                      'scale': 30,\n",
    "                      'maxPixels': 1e9\n",
    "                  })\n",
    "                  task.start()\n",
    "\n",
    "                  # filename= city + \"_Summer\"\n",
    "                  # filename = os.path.join(country, filename)\n",
    "                  # geemap.ee_export_image(collectionSummer, filename=filename, \n",
    "                  # scale=30, region=AOI)\n",
    "                #   geemap.ee_export_image_collection_to_drive(collectionSummer,\n",
    "                #    folder='export', scale=30)\n",
    "                \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error : {e} \")\n",
    "    return  print(f\"Successfully exported the rasters for: {country}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a18760",
   "metadata": {},
   "source": [
    "#### Pick this up after this iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f62062c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for city in cities:\n",
    "#     cities_reprojected = gpd.read_file(Path('shapefile') / (city.replace(\" \", \"_\").lower() + '.shp')).to_crs(epsg = 4326)\n",
    "#     max_value=5\n",
    "#     years =list(str(i) for i in range(2013, 2024)) #years from 2013-2023\n",
    "#     month0 = '04'  # first hottest month  # update for each country\n",
    "#     month1 = '07'  # end of hottest month (note that this is exclusive)  # update for each country\n",
    "#     country= 'BGD' # replace with the relevent country name\n",
    "#     # cities_reprojected = .to_crs({'init': 'epsg:4326'})\n",
    "#     start_exporting= export_landsurface_temperature(country, cities_reprojected, max_value, years, month0, month1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section11'></a>\n",
    "<h5><center> <font color='cyan'> Section 11: Arcpy stats </font>   </center></h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7519eac",
   "metadata": {},
   "source": [
    "***Copy the fathom flood datasets n their respective cities subfolder***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "99c28512",
   "metadata": {},
   "outputs": [],
   "source": [
    "fathom_folder = Path('data') / 'Fathom'\n",
    "floods = {'fluvial_undefended': 'FU',\n",
    "          'pluvial': 'P'}\n",
    "rps = [5, 10, 20, 50, 75, 100, 200, 250, 500, 1000]\n",
    "\n",
    "for city in cities:\n",
    "    for flood in floods:\n",
    "        for rp in rps:\n",
    "            copyfile(fathom_folder / flood / (floods[flood] + '_1in' + str(rp) + '.tif'), \n",
    "                     city / Path('data') / (floods[flood] + '_1in' + str(rp) + '.tif'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71150905",
   "metadata": {},
   "source": [
    "**Built up projection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cae4f5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ssps = ['1', '2', '3']\n",
    "# ssps = ['2', '5']\n",
    "# years = ['2050', '2100']\n",
    "# for city in cities:\n",
    "#     city1 = city.lower().replace(' ', '_')\n",
    "#     city2 = city1.replace('-', '_')\n",
    "#     for flood in floods:\n",
    "#         for ssp in ssps:\n",
    "#             for year in years:\n",
    "#                 ZonalStatisticsAsTable(city + '/data/class_all_' + flood + '.tif', \n",
    "#                                        \"Value\", city + '/data/' + city1 + '_bu_ssp' + ssp + '_' + year + '.tif', \n",
    "#                                        city2 + '_ssp' + ssp + '_' + flood + '_' + year, \"DATA\", \"SUM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section12'></a>\n",
    "<h5><center> <font color='cyan'> Section 12: Package data for sharing</font>   </center></h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4d5d6863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(name):\n",
    "    try:\n",
    "        os.mkdir(name)\n",
    "    except FileExistsError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eaecdb",
   "metadata": {},
   "source": [
    "### Share maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9702b8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssps = ['1', '2', '3']\n",
    "ssps = ['2', '5']\n",
    "map_folders = ['air', 'AOI', 'landslide', 'summer', 'fluvial', 'pluvial']\n",
    "for s in ssps:\n",
    "    for f in ['fu', 'pu']:\n",
    "        for y in ['2050', '2100']:\n",
    "            map_folders.append('bu_proj_' + f + '_ssp' + s + '_' + y)\n",
    "    for t in ['day', 'night']:\n",
    "        map_folders.append('summer_' + t + '_ssp' + s)\n",
    "for y in ['2050', '2100']:\n",
    "    for s in ['', '_RL10']:\n",
    "        map_folders.append('slr' + s + '_' + y)\n",
    "\n",
    "for i in map_folders:\n",
    "    create_folder(Path('maps') / i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430277d0",
   "metadata": {},
   "source": [
    "###### Run the R scripts that creates maps before this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b4eb67b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_file(city, orig_suffix, new_folder = '', new_suffix = ''):\n",
    "    city1 = city.lower().replace(' ', '_')\n",
    "    \n",
    "    if new_folder == '':\n",
    "        new_folder = orig_suffix\n",
    "    if new_suffix == '':\n",
    "        new_suffix = orig_suffix\n",
    "    \n",
    "    try:\n",
    "        copyfile(city / Path('maps') / (city1 + '_' + orig_suffix + '.png'), \n",
    "                 'maps' / Path(new_folder) / (city1 + '_' + new_suffix + '.png'))\n",
    "    except FileNotFoundError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0a9719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b517307f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a28b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab8c2bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2ee946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23e4696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf6d95f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
